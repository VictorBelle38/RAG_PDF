{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as Bibliotecas Necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conectando as Variaveis de Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o arquivo PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/2210.03629v3.pdf\"\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando o Texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator= \"\\n\")\n",
    "docs = text_splitter.split_documents(documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "    docs,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando os Embeddings com Cohere e Criando Vetores com Pinecones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\", cohere_api_key=COHERE_API_KEY)\n",
    "index_name = \"rag-pdf\"\n",
    "\n",
    "vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "    docs, index_name=index_name, embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='821b35dd-264a-447b-bfd5-4a1e7ac9ce87', metadata={'author': '', 'creationdate': '2023-03-13T00:09:11+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'page': 8.0, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data/2210.03629v3.pdf', 'subject': '', 'title': '', 'total_pages': 33.0, 'trapped': '/False'}, page_content='Published as a conference paper at ICLR 2023\\n5 R ELATED WORK\\nLanguage model for reasoning Perhaps the most well-known work of using LLMs for reasoning\\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\\nown “thinking procedure” for problem solving. Several follow-up works have since been performed,\\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\\nof “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\\nﬁnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which ﬁnetunes a LM on\\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ﬁxed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making The strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\\nis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\\nInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\\nﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\\nthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We\\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti'), Document(id='240891bd-0e4f-4877-9acf-49620b7b2705', metadata={'author': '', 'creationdate': '2023-03-13T00:09:11+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'page': 8.0, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data/2210.03629v3.pdf', 'subject': '', 'title': '', 'total_pages': 33.0, 'trapped': '/False'}, page_content='Published as a conference paper at ICLR 2023\\n5 R ELATED WORK\\nLanguage model for reasoning Perhaps the most well-known work of using LLMs for reasoning\\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\\nown “thinking procedure” for problem solving. Several follow-up works have since been performed,\\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\\nof “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\\nﬁnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which ﬁnetunes a LM on\\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ﬁxed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making The strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\\nis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\\nInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\\nﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\\nthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We\\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti'), Document(id='2ebd04e4-db96-4abf-a61a-cbf12b076e62', metadata={'author': '', 'creationdate': '2023-03-13T00:09:11+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'page': 8.0, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data/2210.03629v3.pdf', 'subject': '', 'title': '', 'total_pages': 33.0, 'trapped': '/False'}, page_content='Published as a conference paper at ICLR 2023\\n5 R ELATED WORK\\nLanguage model for reasoning Perhaps the most well-known work of using LLMs for reasoning\\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\\nown “thinking procedure” for problem solving. Several follow-up works have since been performed,\\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\\nof “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\\nﬁnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which ﬁnetunes a LM on\\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ﬁxed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making The strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\\nis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\\nInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\\nﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\\nthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We\\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti'), Document(id='bed92cf2-381b-4c42-93e0-d21d5fb56e06', metadata={'author': '', 'creationdate': '2023-03-13T00:09:11+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'page': 8.0, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data/2210.03629v3.pdf', 'subject': '', 'title': '', 'total_pages': 33.0, 'trapped': '/False'}, page_content='Published as a conference paper at ICLR 2023\\n5 R ELATED WORK\\nLanguage model for reasoning Perhaps the most well-known work of using LLMs for reasoning\\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\\nown “thinking procedure” for problem solving. Several follow-up works have since been performed,\\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\\nof “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\\nﬁnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which ﬁnetunes a LM on\\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ﬁxed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making The strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\\nis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\\nInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\\nﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\\nthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We\\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti')]\n"
     ]
    }
   ],
   "source": [
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "print(vectorstore.similarity_search(\"What is a llm?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a memória com o Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor Bellé\\AppData\\Local\\Temp\\ipykernel_10156\\3353824168.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando CHAT com RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model = \"Gemma2-9b-It\", \n",
    "    groq_api_key = GROQ_API_KEY, \n",
    "    temperature = 0.1, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm, retrieval_qa_chat_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa= create_retrieval_chain(\n",
    "    vectorstore.as_retriever(),\n",
    "    combine_docs_chain  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided text, ReAct is a paradigm that can address the shortcomings of a lack of commonsense reasoning.  \n",
      "\n",
      "It specifically mentions \"ReAct-IM\" which seems to be a specific implementation of the ReAct paradigm. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supondo que você tenha um QA Chain configurado\n",
    "response = qa.invoke({\"input\": \"What is React according to the pdf?\"})\n",
    "print(response['answer'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
